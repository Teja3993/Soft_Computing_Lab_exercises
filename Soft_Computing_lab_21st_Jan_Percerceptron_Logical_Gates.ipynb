{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrnBljhYa8LN0TvqNDyIQo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Teja3993/Soft_Computing_Lab_exercises/blob/main/Soft_Computing_lab_21st_Jan_Percerceptron_Logical_Gates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define AND gate inputs and outputs\n",
        "X_and = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "y_and = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Perceptron Class\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, n_iterations=100):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _step_function(self, x):\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_predicted = self._step_function(linear_output)\n",
        "\n",
        "                # Perceptron learning rule\n",
        "                update = self.learning_rate * (y[idx] - y_predicted)\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return self._step_function(linear_output)\n",
        "\n",
        "# Create and train the Perceptron for AND gate\n",
        "perceptron_and = Perceptron(learning_rate=0.1, n_iterations=10)\n",
        "perceptron_and.fit(X_and, y_and)\n",
        "\n",
        "# Test the Perceptron\n",
        "predictions_and = perceptron_and.predict(X_and)\n",
        "\n",
        "print(\"AND Gate Perceptron Results:\")\n",
        "print(f\"Weights: {perceptron_and.weights}\")\n",
        "print(f\"Bias: {perceptron_and.bias}\")\n",
        "print(\"\\nInput (X):\\n\", X_and)\n",
        "print(\"Target (y):\\n\", y_and)\n",
        "print(\"Predicted (y_pred):\\n\", predictions_and)\n",
        "\n",
        "# Check accuracy\n",
        "accuracy = np.sum(predictions_and == y_and) / len(y_and)\n",
        "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzVSr9pXkLfO",
        "outputId": "a5fe88a3-045a-4d17-bdbf-590be50b2700"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate Perceptron Results:\n",
            "Weights: [0.2 0.1]\n",
            "Bias: -0.20000000000000004\n",
            "\n",
            "Input (X):\n",
            " [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Target (y):\n",
            " [0 0 0 1]\n",
            "Predicted (y_pred):\n",
            " [0 0 0 1]\n",
            "\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3a72539"
      },
      "source": [
        "### Perceptron Implementation using Functions and Loops (without classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c7264cf"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step function (activation function)\n",
        "def step_function(x):\n",
        "    return np.where(x >= 0, 1, 0)\n",
        "\n",
        "# Function to train the perceptron\n",
        "def train_perceptron(X, y, learning_rate=0.1, n_iterations=100):\n",
        "    n_samples, n_features = X.shape\n",
        "    weights = np.zeros(n_features)\n",
        "    bias = 0\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        for idx, x_i in enumerate(X):\n",
        "            linear_output = np.dot(x_i, weights) + bias\n",
        "            y_predicted = step_function(linear_output)\n",
        "\n",
        "            update = learning_rate * (y[idx] - y_predicted)\n",
        "            weights += update * x_i\n",
        "            bias += update\n",
        "    return weights, bias\n",
        "\n",
        "# Function to predict using the trained perceptron\n",
        "def predict_perceptron(X, weights, bias):\n",
        "    linear_output = np.dot(X, weights) + bias\n",
        "    return step_function(linear_output)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03f5b4dc"
      },
      "source": [
        "#### AND Gate Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c3f93d7",
        "outputId": "946796ee-41c8-441b-c63e-d0be64fc751f"
      },
      "source": [
        "# Define AND gate inputs and outputs\n",
        "X_and_func = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "y_and_func = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Train the perceptron for AND gate\n",
        "weights_and, bias_and = train_perceptron(X_and_func, y_and_func, learning_rate=0.1, n_iterations=10)\n",
        "\n",
        "# Test the Perceptron\n",
        "predictions_and_func = predict_perceptron(X_and_func, weights_and, bias_and)\n",
        "\n",
        "print(\"AND Gate Perceptron Results (Functional):\")\n",
        "print(f\"Weights: {weights_and}\")\n",
        "print(f\"Bias: {bias_and}\")\n",
        "print(\"\\nInput (X):\\n\", X_and_func)\n",
        "print(\"Target (y):\\n\", y_and_func)\n",
        "print(\"Predicted (y_pred):\\n\", predictions_and_func)\n",
        "\n",
        "# Check accuracy\n",
        "accuracy_and_func = np.sum(predictions_and_func == y_and_func) / len(y_and_func)\n",
        "print(f\"\\nAccuracy: {accuracy_and_func * 100:.2f}%\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate Perceptron Results (Functional):\n",
            "Weights: [0.2 0.1]\n",
            "Bias: -0.20000000000000004\n",
            "\n",
            "Input (X):\n",
            " [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Target (y):\n",
            " [0 0 0 1]\n",
            "Predicted (y_pred):\n",
            " [0 0 0 1]\n",
            "\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9de30b01"
      },
      "source": [
        "#### OR Gate Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf2462f1",
        "outputId": "40a3b24c-0103-40ef-dd40-2ad9253ebc62"
      },
      "source": [
        "# Define OR gate inputs and outputs\n",
        "X_or_func = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "y_or_func = np.array([0, 1, 1, 1])\n",
        "\n",
        "# Train the perceptron for OR gate\n",
        "weights_or, bias_or = train_perceptron(X_or_func, y_or_func, learning_rate=0.1, n_iterations=10)\n",
        "\n",
        "# Test the Perceptron\n",
        "predictions_or_func = predict_perceptron(X_or_func, weights_or, bias_or)\n",
        "\n",
        "print(\"OR Gate Perceptron Results (Functional):\")\n",
        "print(f\"Weights: {weights_or}\")\n",
        "print(f\"Bias: {bias_or}\")\n",
        "print(\"\\nInput (X):\\n\", X_or_func)\n",
        "print(\"Target (y):\\n\", y_or_func)\n",
        "print(\"Predicted (y_pred):\\n\", predictions_or_func)\n",
        "\n",
        "# Check accuracy\n",
        "accuracy_or_func = np.sum(predictions_or_func == y_or_func) / len(y_or_func)\n",
        "print(f\"\\nAccuracy: {accuracy_or_func * 100:.2f}%\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OR Gate Perceptron Results (Functional):\n",
            "Weights: [0.1 0.1]\n",
            "Bias: -0.1\n",
            "\n",
            "Input (X):\n",
            " [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Target (y):\n",
            " [0 1 1 1]\n",
            "Predicted (y_pred):\n",
            " [0 1 1 1]\n",
            "\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "705d6888"
      },
      "source": [
        "#### XOR Gate Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cea5931",
        "outputId": "4399eb61-13d2-4555-fc86-785410535702"
      },
      "source": [
        "# Define XOR gate inputs and outputs\n",
        "X_xor_func = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "y_xor_func = np.array([0, 1, 1, 0]) # XOR output\n",
        "\n",
        "# Train the perceptron for XOR gate\n",
        "weights_xor, bias_xor = train_perceptron(X_xor_func, y_xor_func, learning_rate=0.1, n_iterations=100) # Increased iterations\n",
        "\n",
        "# Test the Perceptron\n",
        "predictions_xor_func = predict_perceptron(X_xor_func, weights_xor, bias_xor)\n",
        "\n",
        "print(\"XOR Gate Perceptron Results (Functional):\")\n",
        "print(f\"Weights: {weights_xor}\")\n",
        "print(f\"Bias: {bias_xor}\")\n",
        "print(\"\\nInput (X):\\n\", X_xor_func)\n",
        "print(\"Target (y):\\n\", y_xor_func)\n",
        "print(\"Predicted (y_pred):\\n\", predictions_xor_func)\n",
        "\n",
        "# Check accuracy\n",
        "accuracy_xor_func = np.sum(predictions_xor_func == y_xor_func) / len(y_xor_func)\n",
        "print(f\"\\nAccuracy: {accuracy_xor_func * 100:.2f}%\")\n",
        "\n",
        "if accuracy_xor_func < 1.0:\n",
        "    print(\"\\nAs expected, a single perceptron struggles with the XOR gate, as it's not linearly separable. This confirms the earlier observation.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Gate Perceptron Results (Functional):\n",
            "Weights: [-0.1  0. ]\n",
            "Bias: 0.0\n",
            "\n",
            "Input (X):\n",
            " [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Target (y):\n",
            " [0 1 1 0]\n",
            "Predicted (y_pred):\n",
            " [1 1 0 0]\n",
            "\n",
            "Accuracy: 50.00%\n",
            "\n",
            "As expected, a single perceptron struggles with the XOR gate, as it's not linearly separable. This confirms the earlier observation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "X_or = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "y_or = np.array([0, 1, 1, 1])\n",
        "# Create and train the Perceptron for OR gate\n",
        "perceptron_or = Perceptron(learning_rate=0.1, n_iterations=10)\n",
        "perceptron_or.fit(X_or, y_or)\n",
        "\n",
        "# Test the Perceptron\n",
        "predictions_or = perceptron_or.predict(X_or)\n",
        "\n",
        "print(\"OR Gate Perceptron Results:\")\n",
        "print(f\"Weights: {perceptron_or.weights}\")\n",
        "print(f\"Bias: {perceptron_or.bias}\")\n",
        "print(\"\\nInput (X):\\n\", X_or)\n",
        "print(\"Target (y):\\n\", y_or)\n",
        "print(\"Predicted (y_pred):\\n\", predictions_or)\n",
        "\n",
        "# Check accuracy\n",
        "accuracy_or = np.sum(predictions_or == y_or) / len(y_or)\n",
        "print(f\"\\nAccuracy: {accuracy_or * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLf5nsw8k5KE",
        "outputId": "34cdd392-fd90-4562-cf21-b05140f03c42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OR Gate Perceptron Results:\n",
            "Weights: [0.1 0.1]\n",
            "Bias: -0.1\n",
            "\n",
            "Input (X):\n",
            " [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Target (y):\n",
            " [0 1 1 1]\n",
            "Predicted (y_pred):\n",
            " [0 1 1 1]\n",
            "\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define XOR gate inputs and outputs\n",
        "X_xor = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "y_xor = np.array([0, 1, 1, 0]) # XOR output\n",
        "\n",
        "# Create and train the Perceptron for XOR gate\n",
        "# Note: A single perceptron cannot perfectly solve XOR due to non-linear separability.\n",
        "perceptron_xor = Perceptron(learning_rate=0.1, n_iterations=100) # Increased iterations to demonstrate more clearly\n",
        "perceptron_xor.fit(X_xor, y_xor)\n",
        "\n",
        "# Test the Perceptron\n",
        "predictions_xor = perceptron_xor.predict(X_xor)\n",
        "\n",
        "print(\"XOR Gate Perceptron Results:\")\n",
        "print(f\"Weights: {perceptron_xor.weights}\")\n",
        "print(f\"Bias: {perceptron_xor.bias}\")\n",
        "print(\"\\nInput (X):\\n\", X_xor)\n",
        "print(\"Target (y):\\n\", y_xor)\n",
        "print(\"Predicted (y_pred):\\n\", predictions_xor)\n",
        "\n",
        "# Check accuracy\n",
        "accuracy_xor = np.sum(predictions_xor == y_xor) / len(y_xor)\n",
        "print(f\"\\nAccuracy: {accuracy_xor * 100:.2f}%\")\n",
        "\n",
        "if accuracy_xor < 1.0:\n",
        "    print(\"\\nAs expected, a single perceptron struggles with the XOR gate, as it's not linearly separable.\")\n",
        "    print(\"To solve XOR, a multi-layer perceptron (neural network with hidden layers) is typically required.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJVTsvHGlboj",
        "outputId": "ac8a92e1-b568-4bbc-a77f-bc2dcba03c6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Gate Perceptron Results:\n",
            "Weights: [-0.1  0. ]\n",
            "Bias: 0.0\n",
            "\n",
            "Input (X):\n",
            " [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Target (y):\n",
            " [0 1 1 0]\n",
            "Predicted (y_pred):\n",
            " [1 1 0 0]\n",
            "\n",
            "Accuracy: 50.00%\n",
            "\n",
            "As expected, a single perceptron struggles with the XOR gate, as it's not linearly separable.\n",
            "To solve XOR, a multi-layer perceptron (neural network with hidden layers) is typically required.\n"
          ]
        }
      ]
    }
  ]
}